# Guia de In√≠cio R√°pido üöÄ

## Instala√ß√£o

### 1. Clone o reposit√≥rio
```bash
git clone [seu-repositorio]
cd observatorio_araraquara
```

### 2. Crie um ambiente virtual (recomendado)
```bash
python -m venv venv

# Linux/Mac
source venv/bin/activate

# Windows
venv\Scripts\activate
```

### 3. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

## Uso B√°sico

### Coleta de Dados

#### Op√ß√£o 1: Coleta completa (todos os sites e tipos)
```bash
python scrapers/run_all.py
```

#### Op√ß√£o 2: Coleta personalizada
```bash
# Apenas apartamentos √† venda no VivaReal
python scrapers/run_all.py --tipos-imovel apartamento --tipos-transacao venda --sites vivareal --max-paginas 5

# Casas e apartamentos para venda e aluguel
python scrapers/run_all.py --tipos-imovel apartamento casa --tipos-transacao venda aluguel --max-paginas 10
```

#### Op√ß√£o 3: Site espec√≠fico
```bash
# Apenas VivaReal
python scrapers/vivareal_scraper.py --tipo-imovel apartamento --tipo-transacao venda --max-paginas 5

# Apenas OLX
python scrapers/olx_scraper.py --tipo-imovel casa --tipo-transacao aluguel --max-paginas 3
```

### An√°lise de Dados

#### An√°lise r√°pida
```bash
# Usa o arquivo mais recente automaticamente
python analysis/analise_mercado.py
```

#### Relat√≥rio completo com visualiza√ß√µes
```bash
python analysis/analise_mercado.py --relatorio-completo
```

#### An√°lise de arquivo espec√≠fico
```bash
python analysis/analise_mercado.py --arquivo data/raw/vivareal_Araraquara_20260130.csv --relatorio-completo
```

### An√°lise Interativa (Jupyter)

```bash
# Iniciar Jupyter
jupyter notebook

# Abrir o notebook
# notebooks/exploracao_dados.ipynb
```

## Estrutura dos Dados

### Colunas principais coletadas:
- `id_anuncio`: ID √∫nico do an√∫ncio
- `titulo`: T√≠tulo do an√∫ncio
- `tipo_imovel`: apartamento, casa, terreno, etc.
- `tipo_transacao`: venda ou aluguel
- `preco`: Pre√ßo em R$
- `area`: √Årea em m¬≤
- `quartos`: N√∫mero de quartos
- `banheiros`: N√∫mero de banheiros
- `vagas`: Vagas de garagem
- `endereco`: Endere√ßo completo
- `bairro`: Nome do bairro
- `cidade`: Araraquara
- `estado`: SP
- `url`: URL do an√∫ncio original
- `fonte`: Site de origem (vivareal, olx, etc.)
- `data_coleta`: Data e hora da coleta

## Exemplos de An√°lises

### 1. Comparar pre√ßos por bairro
```python
import pandas as pd

df = pd.read_csv('data/raw/seu_arquivo.csv')
preco_por_bairro = df.groupby('bairro')['preco'].mean().sort_values(ascending=False)
print(preco_por_bairro.head(10))
```

### 2. Encontrar im√≥veis mais caros vs. mais baratos
```python
# Mais caros
print(df.nlargest(10, 'preco')[['titulo', 'preco', 'bairro', 'tipo_imovel']])

# Mais baratos
print(df.nsmallest(10, 'preco')[['titulo', 'preco', 'bairro', 'tipo_imovel']])
```

### 3. Calcular amplitude de pre√ßos (desigualdade)
```python
amplitude = df['preco'].max() / df['preco'].min()
print(f"Amplitude: {amplitude:.2f}x")
```

### 4. Analisar pre√ßo por m¬≤
```python
df['preco_m2'] = df['preco'] / df['area']
print(df.groupby('bairro')['preco_m2'].mean().sort_values(ascending=False))
```

## Boas Pr√°ticas

### 1. Rate Limiting
Os scrapers j√° implementam delays entre requisi√ß√µes. Para ajustar:
```python
# Em config.py
REQUEST_DELAY = 3  # segundos entre requisi√ß√µes
```

### 2. Limpeza de Dados
Sempre limpe os dados antes de an√°lises:
```python
from analysis.analise_mercado import AnalisadorMercadoImobiliario

analisador = AnalisadorMercadoImobiliario('data/raw/arquivo.csv')
df_limpo = analisador.limpar_dados()
```

### 3. Coleta Incremental
Para coletar dados ao longo do tempo:
```bash
# Adicione ao cron (Linux) ou Task Scheduler (Windows)
# Executar semanalmente
0 2 * * 0 cd /path/to/observatorio_araraquara && python scrapers/run_all.py
```

## Solu√ß√£o de Problemas

### Erro: "No module named 'requests'"
```bash
pip install -r requirements.txt
```

### Erro: Timeout nas requisi√ß√µes
- Aumente o valor em `config.py`: `TIMEOUT = 60`
- Reduza `max_paginas` nas coletas

### Dados vazios ou incompletos
- Sites mudaram estrutura HTML (comum!)
- Atualize os seletores CSS em `scrapers/`
- Use ferramentas de desenvolvedor do navegador para inspecionar

### "Permission denied" ao salvar
```bash
# Linux/Mac
chmod -R 755 data/

# Ou execute com sudo (n√£o recomendado)
```

## Pr√≥ximos Passos

1. **Geocodifica√ß√£o**: Adicionar coordenadas lat/lon para mapeamento
2. **An√°lise temporal**: Coletar dados periodicamente e acompanhar evolu√ß√£o
3. **Machine Learning**: Prever pre√ßos com base em caracter√≠sticas
4. **Dashboard**: Criar painel interativo com Streamlit ou Dash
5. **API**: Disponibilizar dados via REST API

## Contribuindo

Este √© um projeto educacional inspirado na pesquisa da UNESP. Sinta-se livre para:
- Adicionar novos scrapers (ZapIm√≥veis, Imovelweb, etc.)
- Melhorar an√°lises existentes
- Corrigir bugs
- Documentar melhor

## Recursos Adicionais

- [Tutorial de Web Scraping com BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Artigo Original - Jornal UNESP](https://jornal.unesp.br/2026/01/28/pesquisadores-da-unesp-investigam-efeitos-da-crescente-influencia-do-capital-financeiro-sobre-a-producao-de-moradias-no-brasil/)

## Suporte

Para d√∫vidas ou problemas:
1. Verifique a documenta√ß√£o acima
2. Consulte os logs de erro
3. Abra uma issue no GitHub
